<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>mixup: Beyond Empirical Risk Minimization | Raman Dutt . Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="mixup: Beyond Empirical Risk Minimization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explanation of the paper: mixup: Beyond Empirical Risk Minimization" />
<meta property="og:description" content="Explanation of the paper: mixup: Beyond Empirical Risk Minimization" />
<link rel="canonical" href="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" />
<meta property="og:url" content="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" />
<meta property="og:site_name" content="Raman Dutt . Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-03T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explanation of the paper: mixup: Beyond Empirical Risk Minimization","@type":"BlogPosting","headline":"mixup: Beyond Empirical Risk Minimization","dateModified":"2020-07-03T00:00:00-05:00","datePublished":"2020-07-03T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html"},"url":"https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/portfolio/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://raman1121.github.io/portfolio/feed.xml" title="Raman Dutt . Blog" /><link rel="shortcut icon" type="image/x-icon" href="/portfolio/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>mixup: Beyond Empirical Risk Minimization | Raman Dutt . Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="mixup: Beyond Empirical Risk Minimization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explanation of the paper: mixup: Beyond Empirical Risk Minimization" />
<meta property="og:description" content="Explanation of the paper: mixup: Beyond Empirical Risk Minimization" />
<link rel="canonical" href="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" />
<meta property="og:url" content="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" />
<meta property="og:site_name" content="Raman Dutt . Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-03T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explanation of the paper: mixup: Beyond Empirical Risk Minimization","@type":"BlogPosting","headline":"mixup: Beyond Empirical Risk Minimization","dateModified":"2020-07-03T00:00:00-05:00","datePublished":"2020-07-03T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html"},"url":"https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://raman1121.github.io/portfolio/feed.xml" title="Raman Dutt . Blog" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/portfolio/">Raman Dutt . Blog</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger">
            <a class="page-link" href="/portfolio/images/CV_2021.pdf">Resume</a><a class="page-link" href="/portfolio/about/">About Me</a><a class="page-link" href="/portfolio/search/">Search</a><a class="page-link" href="/portfolio/categories/">Tags</a></div>
        </nav></div>
  </header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">mixup: Beyond Empirical Risk Minimization</h1><p class="page-description">Explanation of the paper: mixup: Beyond Empirical Risk Minimization</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-03T00:00:00-05:00" itemprop="datePublished">
        Jul 3, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/portfolio/categories/#paper summary">paper summary</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#motivation">Motivation</a></li>
<li class="toc-entry toc-h2"><a href="#methodology">Methodology</a></li>
<li class="toc-entry toc-h2"><a href="#experiments">Experiments</a></li>
<li class="toc-entry toc-h2"><a href="#implementations">Implementations</a></li>
</ul><p>If you participate in Kaggle competitions especially related to image classification, you must have definately seen a notebook implementing Mixup probably along with cross validation or any other augmentation method such as CutMix, GridMask, etc which makes this a pretty important concept to have in your toolbox. During my undergraduate thesis when I was working with thermal images, I suspected that my network might suffering due to the presence of corrupted labels in the dataset. Although I had no proof about the presence of such images, I wanted to find something which makes my network more robust and generalizable. This is when I came across <a href="https://arxiv.org/abs/1710.09412">Mixup</a></p>

<p>Mixup is a data augmentation method which comes with a promise to make neural networks more <strong>generalizable</strong>, <strong>reduces memorization of corrupted labels</strong>, <strong>increases robustness</strong> and <strong>stabalizes training of Generative Adversarial Networks (GANs)</strong>. It does so by training the network on combinations of examples in the datasets (images) and their labels.</p>

<h2 id="motivation">
<a class="anchor" href="#motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h2>

<p>A common observed practice in deep learning is that the size of the neural networks increases with increase in size of the dataset with the best example being GPT-3. If you do not know about GPT-3, you are either living under a rock or you are not (active) on Twitter. Basically, GPT-3 is a language model by Open AI which was trained on (almost) the entire web. This model is so huge that it takes 300 GB of space just to store the weights of this model. Now that we have an idea about the size of current models and datasets, lets talk about an interesting result from the classical learning theory which which laid the foundation of modern Machine Learning. This result states that <strong>Convergence of Empirical Risk Minimization (ERM) is guaranteed if the size of Neural Networks does not increase with the size in data.</strong> (ERM is the learning rule which minimizes the average error of deep models during training which is essentialy how we currently train our networks.) This rule is in direct contradiction of how we currently train our models which also challenges the efficacy of ERM.</p>

<p>Memorization tendencies of neural networks are very well covered in the seminal paper <a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a>. Along with this, neural networks have also been accused of behaving very differently in presence of adverserial examples. This observations motivated the authors to look at methods beyond ERM.</p>

<h2 id="methodology">
<a class="anchor" href="#methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methodology</h2>

<p>The authors introduce mixup as a data-agnostic augmentation routine which helps networks generalize better. The equation of mixup is defined below:</p>

<p><img src="/portfolio/images/Mixup/equation.PNG" alt="" title="Mixup Equation to create new training samples"></p>

<p>Here,</p>
<ol>
  <li>x’ and y’ is the new training sample/ label generated by augmentation</li>
  <li>x(i) and x(j) are raw input vectors (images)</li>
  <li>y(i) and y(j) are one-hot encoded labels</li>
  <li>Lambda is the mixup parameter which (in a sense) controls the amount of augmentation</li>
</ol>

<h2 id="experiments">
<a class="anchor" href="#experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments</h2>

<p>The authors perform a number of experiments on different modalities such as image as speech. Even though using gives a state-of-the-art performance on datasets like CIFAR-10, CIFAR-100 and ImageNet-2012. However, in my opinion,the most interesting results are on robustness and generalization of neural networks. Following the experiments conducted by Zhang et al., the authors compare mixup and ERM on memorization of corrupted labels. They take differebt variants of CIFAR-10 dataset having 20, 50 and 80% corrupted labels (thus making generalization difficult). They experiment with ERM, ERM+dropout, mixup, mixup+dropout configurations. Details about dropout probability and training schematics are given in the paper.</p>

<p>Neural networks are often susceptible to adversarial attacks where they behave very differently in presence of such examples in the test set. The authors test the robustness of networks with ERM and mixup agains White Box attacks (using the same model to generate adversarial examples) and Black Box attacks (using first ERM/ mixup model to generate adversarial examples and then using second ERM/mixup model for testing).</p>

<p>In case of white box attacks, mixup models are 2.7 times more robust than ERM models and in case of black box attacks, mixup models 1.25 times more robust. This shows that mixup produces neural networks that are signiﬁcantly more robust than ERM against adversarial examples in white box and black settings without additional overhead compared to ERM.</p>

<h2 id="implementations">
<a class="anchor" href="#implementations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementations</h2>

<p>In order to understand better, one can try implementing or studying an existing implementation of mixup. In this section, I will point towards a few resources using which you can easily use mixup in your training procedures.</p>

<ol>
  <li><a href="https://github.com/yu4u/mixup-generator">Using a training generator (TensorFlow)</a></li>
  <li><a href="https://www.kaggle.com/devbruce/kakr-2019-3rd-eda-imageprep-mixup-cv-keras/notebook">Kaggle Notebook which implements Mixup with Cross Validation</a></li>
  <li><a href="https://www.kaggle.com/code1110/mixup-cutmix-in-keras#Custom-Image-Generator-with-Mixup-&amp;-Cutmix">Kaggle Notebook which implemets mixup and Cutmix</a></li>
  <li><a href="https://www.dlology.com/blog/how-to-do-mixup-training-from-image-files-in-keras/">A blog explaining an open-sourced implementation of mixup</a></li>
</ol>

<p>Thanks for reading! I hope you are able to experiment with this widely-used technique using these resources. I would strongly suggest reading the mixup paper for a proper understanding of this technique and the algorithm behind it.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Raman1121/portfolio"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/portfolio/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/portfolio/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/portfolio/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A collection of my research, thoughts and experiences.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Raman1121" title="Raman1121"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.instagram.com/raman.paradox" title="raman.paradox"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/raman-dutt" title="raman-dutt"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/RamanDutt4" title="RamanDutt4"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
