<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://raman1121.github.io/portfolio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://raman1121.github.io/portfolio/" rel="alternate" type="text/html" /><updated>2021-03-29T13:21:43-05:00</updated><id>https://raman1121.github.io/portfolio/feed.xml</id><title type="html">Raman Dutt . Blog</title><subtitle>A collection of my research, thoughts and experiences.</subtitle><entry><title type="html">Multi-task weak supervision enables anatomically resolved abnormality detection in whole-body FDG-PET/CT</title><link href="https://raman1121.github.io/portfolio/medical%20imaging/2021/03/29/MLT_Weak_Supervision.html" rel="alternate" type="text/html" title="Multi-task weak supervision enables anatomically resolved abnormality detection in whole-body FDG-PET/CT" /><published>2021-03-29T00:00:00-05:00</published><updated>2021-03-29T00:00:00-05:00</updated><id>https://raman1121.github.io/portfolio/medical%20imaging/2021/03/29/MLT_Weak_Supervision</id><content type="html" xml:base="https://raman1121.github.io/portfolio/medical%20imaging/2021/03/29/MLT_Weak_Supervision.html">&lt;p&gt;The following paper by Eyuboglu et al. has recently appeared in Nature Communications. It describes an attention-based, multi-task CNN that detects and localizes abnormalities in whole-body FDG-PET/CT scans. The “weak-supervision” in this framework comes from the process of deriving annotations from a language model trained on free-text radiology reports (a part of the dataset). The authors report the following observations -&lt;/p&gt;

&lt;ol&gt;
    &lt;li&gt; Training in a multi-task fashion enhances the performance. &lt;/li&gt;
    &lt;li&gt; Task-specific attention modules further improve the localization performance. &lt;/li&gt;
    &lt;li&gt; Task-specific attention modules further improve the localization performance. &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;motivations&quot;&gt;Motivations&lt;/h2&gt;
&lt;p&gt;Limited availability of data has been a long-standing roadblock in the context of supervised learning. This has motivated researchers to look beyond strictly-supervised techniques such as self-supervision and weak-supervision. On a similar note, the authors address this challenge by devising a framework where they utilize data from another modality (text) is used to derive annotations.&lt;br /&gt;
Labeled datasets of appropriate size and structure for several diagnostic tasks are hard to find. Moreover, it is a widely known fact that deep learning models fail to generalize to datasets from different distributions. Change in imaging scanner type, adoption of a different post-processing method, etc are some of the common ways which can influence the data distribution and render the model impractical. &lt;br /&gt;
These challenges laid the foundation for this research. The authors have developed a framework which eliminates the need to manually annotate the dataset by inferring annotations from radiology free-text reports.&lt;/p&gt;

&lt;p&gt;In the next section, I will attempt my best to describe some of the major components of this work.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;h3 id=&quot;language-model&quot;&gt;Language Model&lt;/h3&gt;
&lt;p&gt;Instead of relying on the hard-coded rules (supervised annotations), the authors develop a model to predict if an anatomical region is mentioned in case of an abnormal finding. This model is based on well-known BERT architecture.&lt;br /&gt;
Instead of relying on the hard-coded rules (supervised annotations), the authors develop a model to predict if an anatomical region is mentioned in case of an abnormal finding. This model is based on well-known BERT architecture.&lt;br /&gt;
Another interesting step that the authors perform is making BERT more specific to the medical (radiology) domain. BERT is trained on generic English text. In order to prevent the splitting of specialized, domain-specific terms in the reports, the authors develop a greedy algorithm to find the set of 3000 wordpieces that minimize the number of tokens required to reconstruct the reports in our training dataset. Out of these 3000, the words which were not present in BERT’s vocabulary were added by replacing the “unusedX” and non-ASCII tokens provided in the BERT implementation. This allowed them to leverage BERT’s original weights while adding domain-specific tokens.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-regional-ontology&quot;&gt;Creating a Regional Ontology&lt;/h3&gt;

&lt;p&gt;The authors constructed an ontology of 94 regions, expressed as a directed acyclic graph. This contains both high level regions like “chest”, “pelvis”, “neck” along with fine-grained regions like “left lung”, “upper lobe of right lung”. The directed graph is used to establish relationship between these high-level and fine-grained regions. For instance, regions such as “Hilar Lymph Node” would come point towards “Thoracic Lymph Node” which would further point towards the high-level region “Chest”. As we traverse along these edges, we move from fine-grained regions to higher-level regions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/portfolio/images/MLT_WS/ontology.png&quot; alt=&quot;&quot; title=&quot;Figure 1. A graph describing regional ontology relationships [Taken from the original paper]&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-task-learning-with-attention&quot;&gt;Multi-Task learning with Attention&lt;/h3&gt;

&lt;p&gt;Multi-task learning (MLT) was an idea advocated by Rich Caruana in the late 90s. In MLT, our network attempts to solve several related tasks through shared representations. MLT has significantly matured as an active area of research. This &lt;a href=&quot;https://ruder.io/multi-task/&quot;&gt;article&lt;/a&gt; by Sebastian Rudder is an excellent resource to learn more about the field.&lt;br /&gt;
The authors of this paper describe a MLT-inspired network. They perform extensive ablation studies which show that MLT network performs much better than a standard Single Task Network. The model used was a 3D Convolutional Neural Network (CNN) with 26 binary classification task heads and a shared CNN encoder.&lt;br /&gt;
The MLT framework gave a superior performance in several aspects. Classes and anatomical regions with a very few samples (rare diagnoses) were identified with much higher accuracy and confidence using this approach.&lt;br /&gt;
In addition, the authors infuse attention modules on each of the 26 binary classification heads which offered the following advantages -&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt; Improvement in mean AUROC over all 26 tasks &lt;/li&gt;
    &lt;li&gt; Better model interpretability by projecting model's attention distribution to the original image &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding Thoughts&lt;/h2&gt;

&lt;p&gt;The paper was written in a clear language making it easy to read and understand. Some of the unique ideas that had an impact on me are the following -&lt;/p&gt;

&lt;ol&gt;
    &lt;li&gt; Multi Task Learning : I have seen MLT models performing better in some cases. Here, the performance gain is quite significant. This has motivated me to experiment with different MLT approaches for my own experiments (Kaggle and Research) &lt;/li&gt;

    &lt;li&gt; Soft Attention : I have always been a strong advocate of incorporating attention mechanisms particularly for medical imaging problems. This is because medical images have a bodily region of interest (ROI) as compared to natural images having a more global ROI. I am happy to see my thoughts having some practical value. &lt;/li&gt;

    &lt;li&gt; Weak Supervision : One of my own research projects involved weak supervision. I always believed that it offers only marginal advantage. However, this work has shown that weak supervision could offer substantial performance gains given that we have unstructured data present. This is something I would like to try for future Kaggle competitions. &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">The following paper by Eyuboglu et al. has recently appeared in Nature Communications. It describes an attention-based, multi-task CNN that detects and localizes abnormalities in whole-body FDG-PET/CT scans. The “weak-supervision” in this framework comes from the process of deriving annotations from a language model trained on free-text radiology reports (a part of the dataset). The authors report the following observations -</summary></entry><entry><title type="html">Transfusion: Understanding Transfer Learning for Medical Imaging.</title><link href="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/paper%20summary/2020/07/12/Transfusion.html" rel="alternate" type="text/html" title="Transfusion: Understanding Transfer Learning for Medical Imaging." /><published>2020-07-12T00:00:00-05:00</published><updated>2020-07-12T00:00:00-05:00</updated><id>https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/paper%20summary/2020/07/12/Transfusion</id><content type="html" xml:base="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/paper%20summary/2020/07/12/Transfusion.html">&lt;h1 id=&quot;transfusion-understanding-transfer-learning-for-medical-imaging&quot;&gt;Transfusion: Understanding Transfer Learning for Medical Imaging&lt;/h1&gt;

&lt;p&gt;For my undergraduate thesis, I chose to work with Dr. Tavpritesh Sethi at IIIT-Delhi at the intersection of computer vision and medicine, particularly medical imaging. During the project, I had to train several classification models with different strategies in terms of data augmentation, treating imbalance, model pretraining, etc. While experimenting with different pretraining methods, I observed some marked differences in the training dynamics of the network which got me interested in this particular field and led me to this &lt;a href=&quot;https://arxiv.org/abs/1902.07208&quot;&gt;paper&lt;/a&gt; by the Google Brain team.&lt;/p&gt;

&lt;h2 id=&quot;paper-explanation&quot;&gt;Paper Explanation&lt;/h2&gt;

&lt;p&gt;Due to the success of deep learning in medical imaging applications combined with the rapid developments in computer vision, several practices such as using weights of pretrained networks and models on the ImageNet dataset have become de-facto methods for the training procedure. In this paper, the authors have questioned the efficacy of these practices and provide a solid reasoning with the help of exhaustive experiments that there is a need to rethink these procedures when it comes to medical imaging. In addition to this, authors also study the extent of feature reuse accompanied with different weight initialization schemes and provide a scheme of their own.&lt;/p&gt;

&lt;p&gt;Firstly, the authors start by listing out fundamental differences in the dataset used in medical imaging and ImageNet which also serves as a major motivation for this study -&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt; Medical images have a rather confined region of interest as compared to the natural images in ImageNet which have a clear goal subject. &lt;/li&gt;
  &lt;li&gt; Medical datasets are much smaller in size as compared to the ImageNet dataset. &lt;/li&gt;
  &lt;li&gt; Properties such as background illumination, texture and size of images in the two categories of datasets are often very different. &lt;/li&gt;
  &lt;li&gt; Medical tasks often have much fewer number of classes as compared to 1000 classes in ImageNet. &lt;/li&gt; 
&lt;/ol&gt;

&lt;p&gt;These observations like confined region of interest, size, illumination and (possible) over-parameterization of models points to the fact that standard imagenet models and their pretrained weights might be sub-optimal in the medical imaging setting.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;The authors experiment with a variety of model architectures and pretraining schemes to study their affects in feature reuse and overall model performance.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model Architecture&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;ImageNet Models&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Model such as ResNet and Inception&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;CBR Models&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Simple models following Conv2D-BN-ReLU progression&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Weight Initialization Scheme&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Random&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Weights initialized Randomly&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Pretrained from ImageNet&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Pretrained Weights from ImageNet&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In these experiments, it was observed that smaller models (CBR) perform at-par with larger models such as ResNet despite significant differences in size and complexity. We see a similar outcome in experiments performed using random initialization. &lt;strong&gt;These observations raise questions about de-facto practices of using models like ResNet with pretrained weights.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;notable-observations&quot;&gt;Notable Observations&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt; Transfer Learning has a bigger effect with very small amounts of data. &lt;/li&gt;
&lt;li&gt; Transfer primarily helps the large models and smaller models show little difference between transfer and random initialization. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to study the effects on hidden representations, the authors used CCA similarity score between representations learnt by different initialization schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/portfolio/images/Transfusion/CCA-plot.PNG&quot; alt=&quot;&quot; title=&quot;Plotting CCA similarity score wrt different initialization schemes.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above plot reveals a major detail that &lt;strong&gt;representations learnt with random initialization are much more similar to each other than those learnt with pretrained ImageNet weights for larger models,with less of a distinction for smaller models.&lt;/strong&gt; This means that in case of larger models (ResNet50), the representations learnt when trained with random weights are more similar to each other than representations learnt with pretrained weights. In case of smaller models (CBR), the representations learnt with the two initializations do not show much difference. &lt;em&gt;Hence, initialization has a more evident effect of larger models.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;effects-of-transfer-on-feature-reuse&quot;&gt;Effects of Transfer on Feature Reuse&lt;/h2&gt;
&lt;p&gt;Feature reuse in the ability of using features already leant by pretrained models during fine tuning on a downstream (medical) task. An important question that the authors had initially set out to answer was “&lt;em&gt;Where exactly does feature reuse take place using transfer learning?&lt;/em&gt;” From the experiments, it was observed that &lt;strong&gt;feature reuse is mostly limited to bottom (initial) layers.&lt;/strong&gt; As we move towards the end (top) of the network, the difference representations after the two types of initializaions becomes very small indicating that feature reuse becomes independent of the weight initialization scheme.&lt;/p&gt;

&lt;h2 id=&quot;effects-of-transfer-on-convergence-speed&quot;&gt;Effects of Transfer on Convergence Speed&lt;/h2&gt;
&lt;p&gt;Along with other observations, experiments revealed that using pretrained weights offers better convergence speedup which essentially is lesser time/ number of steps to train our model. The authors report that this convergence speedup is due to better &lt;strong&gt;weight scaling&lt;/strong&gt; provided by the pretrained weights. To support this, they performed an experiment where they initialized the weights in a random fashion but with the mean and standard deviation of pretrained weights &lt;em&gt;(thus providing the same weight scaling)&lt;/em&gt;. Training is this setting demonstrated a faster convergence which concludes that &lt;strong&gt;“Transfer learning contributes much more significantly in convergence speed than feature reuse.”&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;weight-transfusion---hybrid-approaches&quot;&gt;Weight Transfusion -&amp;gt; Hybrid Approaches&lt;/h2&gt;
&lt;p&gt;The authors perform a weight transfusion experiment where they “transfer” a continuous set of pretrained weights for one part of the network and randomly initialize rest of the network and then training on the medical task. The results show that simply using pretrained weights for a small segment of the network gives similar performance as full transfer learning.&lt;/p&gt;

&lt;p&gt;This leads to the conclusion that a hybrid training strategy needs to be devised where the network is initialized with pretrained weights upto a certain point, ‘Block2’ in ResNet50 for instance and redesiging the top of the network to be more lightweight, initializing these layers randomly, and training this new model end to end.&lt;/p&gt;

&lt;p&gt;Synthetic filters such as Gabor filters can also be used to initialize the first convolution layer with random initialization for rest of the model.&lt;/p&gt;</content><author><name></name></author><summary type="html">Transfusion: Understanding Transfer Learning for Medical Imaging</summary></entry><entry><title type="html">mixup: Beyond Empirical Risk Minimization</title><link href="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html" rel="alternate" type="text/html" title="mixup: Beyond Empirical Risk Minimization" /><published>2020-07-03T00:00:00-05:00</published><updated>2020-07-03T00:00:00-05:00</updated><id>https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization</id><content type="html" xml:base="https://raman1121.github.io/portfolio/paper%20summary/2020/07/03/Mixup-Beyond-Empirical-Risk-Minimization.html">&lt;p&gt;If you participate in Kaggle competitions especially related to image classification, you must have definately seen a notebook implementing Mixup probably along with cross validation or any other augmentation method such as CutMix, GridMask, etc which makes this a pretty important concept to have in your toolbox. During my undergraduate thesis when I was working with thermal images, I suspected that my network might suffering due to the presence of corrupted labels in the dataset. Although I had no proof about the presence of such images, I wanted to find something which makes my network more robust and generalizable. This is when I came across &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;Mixup&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mixup is a data augmentation method which comes with a promise to make neural networks more &lt;strong&gt;generalizable&lt;/strong&gt;, &lt;strong&gt;reduces memorization of corrupted labels&lt;/strong&gt;, &lt;strong&gt;increases robustness&lt;/strong&gt; and &lt;strong&gt;stabalizes training of Generative Adversarial Networks (GANs)&lt;/strong&gt;. It does so by training the network on combinations of examples in the datasets (images) and their labels.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;A common observed practice in deep learning is that the size of the neural networks increases with increase in size of the dataset with the best example being GPT-3. If you do not know about GPT-3, you are either living under a rock or you are not (active) on Twitter. Basically, GPT-3 is a language model by Open AI which was trained on (almost) the entire web. This model is so huge that it takes 300 GB of space just to store the weights of this model. Now that we have an idea about the size of current models and datasets, lets talk about an interesting result from the classical learning theory which which laid the foundation of modern Machine Learning. This result states that &lt;strong&gt;Convergence of Empirical Risk Minimization (ERM) is guaranteed if the size of Neural Networks does not increase with the size in data.&lt;/strong&gt; (ERM is the learning rule which minimizes the average error of deep models during training which is essentialy how we currently train our networks.) This rule is in direct contradiction of how we currently train our models which also challenges the efficacy of ERM.&lt;/p&gt;

&lt;p&gt;Memorization tendencies of neural networks are very well covered in the seminal paper &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;Understanding deep learning requires rethinking generalization&lt;/a&gt;. Along with this, neural networks have also been accused of behaving very differently in presence of adverserial examples. This observations motivated the authors to look at methods beyond ERM.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;The authors introduce mixup as a data-agnostic augmentation routine which helps networks generalize better. The equation of mixup is defined below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/portfolio/images/Mixup/equation.PNG&quot; alt=&quot;&quot; title=&quot;Mixup Equation to create new training samples&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;x’ and y’ is the new training sample/ label generated by augmentation&lt;/li&gt;
  &lt;li&gt;x(i) and x(j) are raw input vectors (images)&lt;/li&gt;
  &lt;li&gt;y(i) and y(j) are one-hot encoded labels&lt;/li&gt;
  &lt;li&gt;Lambda is the mixup parameter which (in a sense) controls the amount of augmentation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;The authors perform a number of experiments on different modalities such as image as speech. Even though using gives a state-of-the-art performance on datasets like CIFAR-10, CIFAR-100 and ImageNet-2012. However, in my opinion,the most interesting results are on robustness and generalization of neural networks. Following the experiments conducted by Zhang et al., the authors compare mixup and ERM on memorization of corrupted labels. They take differebt variants of CIFAR-10 dataset having 20, 50 and 80% corrupted labels (thus making generalization difficult). They experiment with ERM, ERM+dropout, mixup, mixup+dropout configurations. Details about dropout probability and training schematics are given in the paper.&lt;/p&gt;

&lt;p&gt;Neural networks are often susceptible to adversarial attacks where they behave very differently in presence of such examples in the test set. The authors test the robustness of networks with ERM and mixup agains White Box attacks (using the same model to generate adversarial examples) and Black Box attacks (using first ERM/ mixup model to generate adversarial examples and then using second ERM/mixup model for testing).&lt;/p&gt;

&lt;p&gt;In case of white box attacks, mixup models are 2.7 times more robust than ERM models and in case of black box attacks, mixup models 1.25 times more robust. This shows that mixup produces neural networks that are signiﬁcantly more robust than ERM against adversarial examples in white box and black settings without additional overhead compared to ERM.&lt;/p&gt;

&lt;h2 id=&quot;implementations&quot;&gt;Implementations&lt;/h2&gt;

&lt;p&gt;In order to understand better, one can try implementing or studying an existing implementation of mixup. In this section, I will point towards a few resources using which you can easily use mixup in your training procedures.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yu4u/mixup-generator&quot;&gt;Using a training generator (TensorFlow)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/devbruce/kakr-2019-3rd-eda-imageprep-mixup-cv-keras/notebook&quot;&gt;Kaggle Notebook which implements Mixup with Cross Validation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/code1110/mixup-cutmix-in-keras#Custom-Image-Generator-with-Mixup-&amp;amp;-Cutmix&quot;&gt;Kaggle Notebook which implemets mixup and Cutmix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dlology.com/blog/how-to-do-mixup-training-from-image-files-in-keras/&quot;&gt;A blog explaining an open-sourced implementation of mixup&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks for reading! I hope you are able to experiment with this widely-used technique using these resources. I would strongly suggest reading the mixup paper for a proper understanding of this technique and the algorithm behind it.&lt;/p&gt;</content><author><name></name></author><summary type="html">If you participate in Kaggle competitions especially related to image classification, you must have definately seen a notebook implementing Mixup probably along with cross validation or any other augmentation method such as CutMix, GridMask, etc which makes this a pretty important concept to have in your toolbox. During my undergraduate thesis when I was working with thermal images, I suspected that my network might suffering due to the presence of corrupted labels in the dataset. Although I had no proof about the presence of such images, I wanted to find something which makes my network more robust and generalizable. This is when I came across Mixup</summary></entry></feed>