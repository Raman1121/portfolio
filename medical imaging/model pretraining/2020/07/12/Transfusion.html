<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Transfusion: Understanding Transfer Learning for Medical Imaging. | Raman Dutt . Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Transfusion: Understanding Transfer Learning for Medical Imaging." />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging." />
<meta property="og:description" content="Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging." />
<link rel="canonical" href="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html" />
<meta property="og:url" content="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html" />
<meta property="og:site_name" content="Raman Dutt . Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging.","mainEntityOfPage":{"@type":"WebPage","@id":"https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html"},"@type":"BlogPosting","url":"https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html","headline":"Transfusion: Understanding Transfer Learning for Medical Imaging.","dateModified":"2020-07-12T00:00:00-05:00","datePublished":"2020-07-12T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/portfolio/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://raman1121.github.io/portfolio/feed.xml" title="Raman Dutt . Blog" /><link rel="shortcut icon" type="image/x-icon" href="/portfolio/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Transfusion: Understanding Transfer Learning for Medical Imaging. | Raman Dutt . Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Transfusion: Understanding Transfer Learning for Medical Imaging." />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging." />
<meta property="og:description" content="Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging." />
<link rel="canonical" href="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html" />
<meta property="og:url" content="https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html" />
<meta property="og:site_name" content="Raman Dutt . Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging.","mainEntityOfPage":{"@type":"WebPage","@id":"https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html"},"@type":"BlogPosting","url":"https://raman1121.github.io/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html","headline":"Transfusion: Understanding Transfer Learning for Medical Imaging.","dateModified":"2020-07-12T00:00:00-05:00","datePublished":"2020-07-12T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://raman1121.github.io/portfolio/feed.xml" title="Raman Dutt . Blog" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/portfolio/">Raman Dutt . Blog</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger">
            <a class="page-link" href="/portfolio/images/Resume-Updated.pdf">Resume</a><a class="page-link" href="/portfolio/about/">About Me</a><a class="page-link" href="/portfolio/search/">Search</a><a class="page-link" href="/portfolio/categories/">Tags</a></div>
        </nav></div>
  </header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transfusion: Understanding Transfer Learning for Medical Imaging.</h1><p class="page-description">Explanation of the paper: Transfusion: Understanding Transfer Learning for Medical Imaging.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-12T00:00:00-05:00" itemprop="datePublished">
        Jul 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/portfolio/categories/#medical imaging">medical imaging</a>
        &nbsp;
      
        <a class="category-tags-link" href="/portfolio/categories/#model pretraining">model pretraining</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#transfusion-understanding-transfer-learning-for-medical-imaging">Transfusion: Understanding Transfer Learning for Medical Imaging</a>
<ul>
<li class="toc-entry toc-h2"><a href="#paper-explanation">Paper Explanation</a></li>
<li class="toc-entry toc-h2"><a href="#experiments">Experiments</a></li>
<li class="toc-entry toc-h2"><a href="#notable-observations">Notable Observations</a></li>
<li class="toc-entry toc-h2"><a href="#effects-of-transfer-on-feature-reuse">Effects of Transfer on Feature Reuse</a></li>
<li class="toc-entry toc-h2"><a href="#effects-of-transfer-on-convergence-speed">Effects of Transfer on Convergence Speed</a></li>
<li class="toc-entry toc-h2"><a href="#weight-transfusion---hybrid-approaches">Weight Transfusion -&gt; Hybrid Approaches</a></li>
</ul>
</li>
</ul><h1 id="transfusion-understanding-transfer-learning-for-medical-imaging">
<a class="anchor" href="#transfusion-understanding-transfer-learning-for-medical-imaging" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfusion: Understanding Transfer Learning for Medical Imaging</h1>

<p>For my undergraduate thesis, I chose to work with Dr. Tavpritesh Sethi at IIIT-Delhi at the intersection of computer vision and medicine, particularly medical imaging. During the project, I had to train several classification models with different strategies in terms of data augmentation, treating imbalance, model pretraining, etc. While experimenting with different pretraining methods, I observed some marked differences in the training dynamics of the network which got me interested in this particular field and led me to this <a href="https://arxiv.org/abs/1902.07208">paper</a> by the Google Brain team.</p>

<h2 id="paper-explanation">
<a class="anchor" href="#paper-explanation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Paper Explanation</h2>

<p>Due to the success of deep learning in medical imaging applications combined with the rapid developments in computer vision, several practices such as using weights of pretrained networks and models on the ImageNet dataset have become de-facto methods for the training procedure. In this paper, the authors have questioned the efficacy of these practices and provide a solid reasoning with the help of exhaustive experiments that there is a need to rethink these procedures when it comes to medical imaging. In addition to this, authors also study the extent of feature reuse accompanied with different weight initialization schemes and provide a scheme of their own.</p>

<p>Firstly, the authors start by listing out fundamental differences in the dataset used in medical imaging and ImageNet which also serves as a major motivation for this study -</p>
<ol>
  <li> Medical images have a rather confined region of interest as compared to the natural images in ImageNet which have a clear goal subject. </li>
  <li> Medical datasets are much smaller in size as compared to the ImageNet dataset. </li>
  <li> Properties such as background illumination, texture and size of images in the two categories of datasets are often very different. </li>
  <li> Medical tasks often have much fewer number of classes as compared to 1000 classes in ImageNet. </li> 
</ol>

<p>These observations like confined region of interest, size, illumination and (possible) over-parameterization of models points to the fact that standard imagenet models and their pretrained weights might be sub-optimal in the medical imaging setting.</p>

<h2 id="experiments">
<a class="anchor" href="#experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments</h2>

<p>The authors experiment with a variety of model architectures and pretraining schemes to study their affects in feature reuse and overall model performance.</p>

<table>
  <thead>
    <tr>
      <th>Model Architecture</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ImageNet Models</strong></td>
      <td>Models such as ResNet50 and Inception which were originally developed for the ImageNet dataset.</td>
    </tr>
    <tr>
      <td><strong>CBR Models</strong></td>
      <td>Simple models (of varying sizes) which follow a progression of Conv2D-BN-ReLU layers</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Weight Initialization Scheme</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Random</strong></td>
      <td>Model weights initialized with Random Initialization</td>
    </tr>
    <tr>
      <td><strong>Pretrained from ImageNet</strong></td>
      <td>Models weights obtained from pretrained networks on ImageNet</td>
    </tr>
  </tbody>
</table>

<p>In these experiments, it was observed that smaller models (CBR) perform at-par with larger models such as ResNet despite significant differences in size and complexity. We see a similar outcome in experiments performed using random initialization. <strong>These observations raise questions about de-facto practices of using models like ResNet with pretrained weights.</strong></p>

<h2 id="notable-observations">
<a class="anchor" href="#notable-observations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notable Observations</h2>

<ul>
<li> Transfer Learning has a bigger effect with very small amounts of data. </li>
<li> Transfer primarily helps the large models and smaller models show little difference between transfer and random initialization. </li>
</ul>

<p>In order to study the effects on hidden representations, the authors used CCA similarity score between representations learnt by different initialization schemes.</p>

<p><img src="/portfolio/images/Transfusion/CCA-plot.PNG" alt="" title="Plotting CCA similarity score wrt different initialization schemes."></p>

<p>The above plot reveals a major detail that <strong>representations learnt with random initialization are much more similar to each other than those learnt with pretrained ImageNet weights for larger models,with less of a distinction for smaller models.</strong> This means that in case of larger models (ResNet50), the representations learnt when trained with random weights are more similar to each other than representations learnt with pretrained weights. In case of smaller models (CBR), the representations learnt with the two initializations do not show much difference. <em>Hence, initialization has a more evident effect of larger models.</em></p>

<h2 id="effects-of-transfer-on-feature-reuse">
<a class="anchor" href="#effects-of-transfer-on-feature-reuse" aria-hidden="true"><span class="octicon octicon-link"></span></a>Effects of Transfer on Feature Reuse</h2>
<p>Feature reuse in the ability of using features already leant by pretrained models during fine tuning on a downstream (medical) task. An important question that the authors had initially set out to answer was “<em>Where exactly does feature reuse take place using transfer learning?</em>” From the experiments, it was observed that <strong>feature reuse is mostly limited to bottom (initial) layers.</strong> As we move towards the end (top) of the network, the difference representations after the two types of initializaions becomes very small indicating that feature reuse becomes independent of the weight initialization scheme.</p>

<h2 id="effects-of-transfer-on-convergence-speed">
<a class="anchor" href="#effects-of-transfer-on-convergence-speed" aria-hidden="true"><span class="octicon octicon-link"></span></a>Effects of Transfer on Convergence Speed</h2>
<p>Along with other observations, experiments revealed that using pretrained weights offers better convergence speedup which essentially is lesser time/ number of steps to train our model. The authors report that this convergence speedup is due to better <strong>weight scaling</strong> provided by the pretrained weights. To support this, they performed an experiment where they initialized the weights in a random fashion but with the mean and standard deviation of pretrained weights <em>(thus providing the same weight scaling)</em>. Training is this setting demonstrated a faster convergence which concludes that <strong>“Transfer learning contributes much more significantly in convergence speed than feature reuse.”</strong></p>

<h2 id="weight-transfusion---hybrid-approaches">
<a class="anchor" href="#weight-transfusion---hybrid-approaches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Transfusion -&gt; Hybrid Approaches</h2>
<p>The authors perform a weight transfusion experiment where they “transfer” a continuous set of pretrained weights for one part of the network and randomly initialize rest of the network and then training on the medical task. The results show that simply using pretrained weights for a small segment of the network gives similar performance as full transfer learning.</p>

<p>This leads to the conclusion that a hybrid training strategy needs to be devised where the network is initialized with pretrained weights upto a certain point, ‘Block2’ in ResNet50 for instance and redesiging the top of the network to be more lightweight, initializing these layers randomly, and training this new model end to end.</p>

<p>Synthetic filters such as Gabor filters can also be used to initialize the first convolution layer with random initialization for rest of the model.</p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Raman1121/portfolio"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/portfolio/medical%20imaging/model%20pretraining/2020/07/12/Transfusion.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/portfolio/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/portfolio/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/portfolio/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A collection of my research, thoughts and experiences.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Raman1121" title="Raman1121"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.instagram.com/raman.paradox" title="raman.paradox"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/raman-dutt" title="raman-dutt"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/RamanDutt4" title="RamanDutt4"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
