---
toc: true
layout: post
description: "Explanation of the paper: Neural Data Server - A Large Scale Search Engine for Transfer Learning Data"
categories: [model pretraining]
title: "Neural Data Server - A Large Scale Search Engine for Transfer Learning Data"
comments: true
---

# Main Ideas

<ul>
    <li> The authors introduce an idea to recommend ideal/ more suitable datasets for pretraining according to the client's downstream task. They do this by indexing a variety of large, popular datasets (ImageNet, CityScapes, etc) on a server. The users query this server with their own target dataset and the server recommends (a subset) of indexed datasets to the user for pretraining. </li>
    
    <li> The assumption is that the user requires only a subset of the indexed datasets which is most relevant to the downstream task. This is backed by past research which shows that more pretraining data DOES NOT lead to a better performance. Instead, smaller, more relevant datasets lead to better representations which further aid the performance on downstream tasks. </li>

</ul>